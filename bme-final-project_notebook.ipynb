{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile IntmdSequential.py\n\nimport torch.nn as nn\n\n\nclass IntermediateSequential(nn.Sequential):\n    def __init__(self, *args, return_intermediate=True):\n        super().__init__(*args)\n        self.return_intermediate = return_intermediate\n\n    def forward(self, input):\n        if not self.return_intermediate:\n            return super().forward(input)\n\n        intermediate_outputs = {}\n        output = input\n        for name, module in self.named_children():\n            output = intermediate_outputs[name] = module(output)\n\n        return output, intermediate_outputs\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:18.897566Z","iopub.execute_input":"2022-05-11T00:40:18.897925Z","iopub.status.idle":"2022-05-11T00:40:18.929575Z","shell.execute_reply.started":"2022-05-11T00:40:18.897827Z","shell.execute_reply":"2022-05-11T00:40:18.928802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile PositionalEncoding.py\n\nimport torch\nimport torch.nn as nn\n\nclass FixedPositionalEncoding(nn.Module):\n    def __init__(self, embedding_dim, max_length=512):\n        super(FixedPositionalEncoding, self).__init__()\n\n        pe = torch.zeros(max_length, embedding_dim)\n        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(\n            torch.arange(0, embedding_dim, 2).float()\n            * (-torch.log(torch.tensor(10000.0)) / embedding_dim)\n        )\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        x = x + self.pe[: x.size(0), :]\n        return x\n\n\nclass LearnedPositionalEncoding(nn.Module):\n    def __init__(self, max_position_embeddings, embedding_dim, seq_length):\n        super(LearnedPositionalEncoding, self).__init__()\n\n        self.position_embeddings = nn.Parameter(torch.zeros(1, 1728, 512)) #8x\n\n    def forward(self, x, position_ids=None):\n\n        position_embeddings = self.position_embeddings\n        return x + position_embeddings\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:18.93098Z","iopub.execute_input":"2022-05-11T00:40:18.931289Z","iopub.status.idle":"2022-05-11T00:40:18.938051Z","shell.execute_reply.started":"2022-05-11T00:40:18.931254Z","shell.execute_reply":"2022-05-11T00:40:18.937289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile TABS_Model.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom Transformer import TransformerModel\nfrom PositionalEncoding import LearnedPositionalEncoding\n\nclass up_conv_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(up_conv_3D, self).__init__()\n        self.up = nn.Sequential(\n            nn.Upsample(scale_factor = 2),\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            # nn.BatchNorm3d(ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.up(x)\n        return x\n\n\nclass conv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(conv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n\n    def forward(self,x):\n        x = self.conv(x)\n        return x\n\nclass resconv_block_3D(nn.Module):\n    def __init__(self, ch_in, ch_out):\n        super(resconv_block_3D, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv3d(ch_in, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(ch_out, ch_out, kernel_size = 3, stride = 1, padding = 1, bias = True),\n            nn.GroupNorm(8, ch_out),\n            nn.ReLU(inplace = True)\n        )\n        self.Conv_1x1 = nn.Conv3d(ch_in, ch_out, kernel_size = 1, stride = 1, padding = 0)\n\n    def forward(self,x):\n        residual = self.Conv_1x1(x)\n        x = self.conv(x)\n        return residual + x\n\n\nclass SE_block_3D(nn.Module):\n    \"\"\"\n    3D extension of Squeeze-and-Excitation (SE) block adapted from:\n     Rickmann et. al., `Project & Excite' Modules for Segmentation of Volumetric Medical Scans\n     https://arxiv.org/abs/1906.04649\n    \"\"\"\n    def __init__(self, ch_in, reduction_ratio=8):\n        super(SE_block_3D, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool3d(1)\n        self.reduction_ratio = reduction_ratio\n        self.fc = nn.Sequential(\n            nn.Linear(ch_in, ch_in // reduction_ratio, bias=True),\n            nn.ReLU(inplace=True),\n            nn.Linear(ch_in // reduction_ratio, ch_in, bias=True),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        batch_size, channels, _, _, _ = x.size()\n        \n        # Squeezing\n        squeeze_tensor = self.avg_pool(x).view(batch_size, channels)\n        # Excitation\n        squeeze_tensor = self.fc(squeeze_tensor).view(batch_size, channels, 1, 1, 1)\n        return x * squeeze_tensor.expand_as(x)\n    \nclass PE_block(nn.Module):\n    \"\"\"\n    Project-and-Excite (PE) block adapted from:\n     Rickmann et. al., `Project & Excite' Modules for Segmentation of Volumetric Medical Scans\n     https://arxiv.org/abs/1906.04649\n    \"\"\"\n    def __init__(self, ch_in, reduction_ratio=8):\n        super(PE_block, self).__init__()\n        self.reduction_ratio = reduction_ratio\n        self.fc = nn.Sequential(\n            nn.Conv3d(in_channels=ch_in, out_channels= ch_in // reduction_ratio, kernel_size=1, stride=1),\n            nn.ReLU(inplace = True),\n            nn.Conv3d(in_channels=ch_in // reduction_ratio, out_channels= ch_in, kernel_size=1, stride=1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        batch_size, channels, depth, height, width = x.size()\n        \n        # Projection\n        squeeze_tensor_w = F.adaptive_avg_pool3d(x, (1, 1, width))\n        squeeze_tensor_h = F.adaptive_avg_pool3d(x, (1, height, 1))\n        squeeze_tensor_d = F.adaptive_avg_pool3d(x, (depth, 1, 1))\n        \n        final_squeeze_tensor = sum([squeeze_tensor_w.view(batch_size, channels, 1, 1, width),\n                                    squeeze_tensor_h.view(batch_size, channels, 1, height, 1),\n                                    squeeze_tensor_d.view(batch_size, channels, depth, 1, 1)])\n        # Excitation\n        final_squeeze_tensor = self.fc(final_squeeze_tensor)\n        return x * final_squeeze_tensor.expand_as(x)\n\nclass TABS(nn.Module):\n    def __init__(\n        self,\n        img_dim = 192,\n        patch_dim = 8,\n        img_ch = 1,\n        output_ch = 3,\n        embedding_dim = 512,\n        num_heads = 8,\n        num_layers = 4,\n        hidden_dim = 1728,\n        dropout_rate = 0.1,\n        attn_dropout_rate = 0.1,\n        ):\n        super(TABS,self).__init__()\n\n        self.Maxpool = nn.MaxPool3d(kernel_size=2,stride=2)\n\n        self.Conv1 = resconv_block_3D(ch_in=img_ch,ch_out=8)\n\n        self.Conv2 = resconv_block_3D(ch_in=8,ch_out=16)\n\n        self.Conv3 = resconv_block_3D(ch_in=16,ch_out=32)\n\n        self.Conv4 = resconv_block_3D(ch_in=32,ch_out=64)\n\n        self.Conv5 = resconv_block_3D(ch_in=64,ch_out=128)\n\n        self.Up5 = up_conv_3D(ch_in=128,ch_out=64)\n        self.Up_conv5 = resconv_block_3D(ch_in=128, ch_out=64)\n\n        self.Up4 = up_conv_3D(ch_in=64,ch_out=32)\n        self.Up_conv4 = resconv_block_3D(ch_in=64, ch_out=32)\n\n        self.Up3 = up_conv_3D(ch_in=32,ch_out=16)\n        self.Up_conv3 = resconv_block_3D(ch_in=32, ch_out=16)\n\n        self.Up2 = up_conv_3D(ch_in=16,ch_out=8)\n        self.Up_conv2 = resconv_block_3D(ch_in=16, ch_out=8)\n\n        self.Conv_1x1 = nn.Conv3d(8,output_ch,kernel_size=1,stride=1,padding=0)\n        self.gn = nn.GroupNorm(8, 128)\n        self.relu = nn.ReLU(inplace=True)\n\n        self.num_patches = int((img_dim // patch_dim) ** 3)\n        self.seq_length = self.num_patches\n        self.flatten_dim = 128 * img_ch\n\n        self.position_encoding = LearnedPositionalEncoding(\n            self.seq_length, embedding_dim, self.seq_length\n        )\n\n        self.act = nn.Softmax(dim=1)\n\n        self.reshaped_conv = conv_block_3D(512, 128)\n\n        self.transformer = TransformerModel(\n            embedding_dim,\n            num_layers,\n            num_heads,\n            hidden_dim,\n\n            dropout_rate,\n            attn_dropout_rate,\n        )\n\n        self.conv_x = nn.Conv3d(\n            128,\n            embedding_dim,\n            kernel_size=3,\n            stride=1,\n            padding=1\n            )\n\n        self.pre_head_ln = nn.LayerNorm(embedding_dim)\n\n        self.img_dim = 192\n        self.patch_dim = 8\n        self.img_ch = 1\n        self.output_ch = 3\n        self.embedding_dim = 512\n        \n        self.SE = SE_block_3D(ch_in=128) # Squeeze-Excitation\n#         self.PE = PE_block(ch_in=128) # Projection-Excitation\n\n    def forward(self,x):\n        # encoding path\n        x1 = self.Conv1(x)\n\n        x2 = self.Maxpool(x1)\n        x2 = self.Conv2(x2)\n\n        x3 = self.Maxpool(x2)\n        x3 = self.Conv3(x3)\n\n        x4 = self.Maxpool(x3)\n        x4 = self.Conv4(x4)\n\n        x5 = self.Maxpool(x4)\n        x = self.Conv5(x5)\n        \n        x = self.SE(x) # Squeeze-Excitation\n#         x = self.PE(x) # Projection-Excitation\n        \n        x = self.gn(x)\n        x = self.relu(x)\n        x = self.conv_x(x)\n\n        x = x.permute(0, 2, 3, 4, 1).contiguous()\n        x = x.view(x.size(0), -1, self.embedding_dim)\n\n        x = self.position_encoding(x)\n\n        x, intmd_x = self.transformer(x)\n        x = self.pre_head_ln(x)\n\n        encoder_outputs = {}\n        all_keys = []\n        for i in [1, 2, 3, 4]:\n            val = str(2 * i - 1)\n            _key = 'Z' + str(i)\n            all_keys.append(_key)\n            encoder_outputs[_key] = intmd_x[val]\n        all_keys.reverse()\n\n        x = encoder_outputs[all_keys[0]]\n        x = self._reshape_output(x)\n        x = self.reshaped_conv(x)\n\n        d5 = self.Up5(x)\n        d5 = torch.cat((x4,d5),dim=1)\n        d5 = self.Up_conv5(d5)\n\n        d4 = self.Up4(d5)\n        d4 = torch.cat((x3,d4),dim=1)\n        d4 = self.Up_conv4(d4)\n\n        d3 = self.Up3(d4)\n        d3 = torch.cat((x2,d3),dim=1)\n        d3 = self.Up_conv3(d3)\n\n        d2 = self.Up2(d3)\n        d2 = torch.cat((x1,d2),dim=1)\n        d2 = self.Up_conv2(d2)\n\n        d1 = self.Conv_1x1(d2)\n\n        d1 = self.act(d1)\n\n        return d1\n\n    def _reshape_output(self, x):\n        x = x.view(\n            x.size(0),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            int(self.img_dim//2 / self.patch_dim),\n            self.embedding_dim,\n        )\n        x = x.permute(0, 4, 1, 2, 3).contiguous()\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:18.96126Z","iopub.execute_input":"2022-05-11T00:40:18.961477Z","iopub.status.idle":"2022-05-11T00:40:18.973601Z","shell.execute_reply.started":"2022-05-11T00:40:18.961453Z","shell.execute_reply":"2022-05-11T00:40:18.972832Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile Transformer.py\n\nimport torch.nn as nn\nfrom IntmdSequential import IntermediateSequential\n\n\nclass SelfAttention(nn.Module):\n    def __init__(\n        self, dim, heads=8, qkv_bias=False, qk_scale=None, dropout_rate=0.0\n    ):\n        super().__init__()\n        self.num_heads = heads\n        head_dim = dim // heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(dropout_rate)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(dropout_rate)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = (\n            self.qkv(x)\n            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n            .permute(2, 0, 3, 1, 4)\n        )\n        q, k, v = (\n            qkv[0],\n            qkv[1],\n            qkv[2],\n        )  # make torchscript happy (cannot use tensor as tuple)\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x\n\n\nclass Residual(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(x) + x\n\n\nclass PreNorm(nn.Module):\n    def __init__(self, dim, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(self.norm(x))\n\n\nclass PreNormDrop(nn.Module):\n    def __init__(self, dim, dropout_rate, fn):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.dropout = nn.Dropout(p=dropout_rate)\n        self.fn = fn\n\n    def forward(self, x):\n        return self.dropout(self.fn(self.norm(x)))\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout_rate):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(p=dropout_rate),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(p=dropout_rate),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass TransformerModel(nn.Module):\n    def __init__(\n        self,\n        dim,\n        depth,\n        heads,\n        mlp_dim,\n        dropout_rate=0.1,\n        attn_dropout_rate=0.1,\n    ):\n        super().__init__()\n        layers = []\n        for _ in range(depth):\n            layers.extend(\n                [\n                    Residual(\n                        PreNormDrop(\n                            dim,\n                            dropout_rate,\n                            SelfAttention(dim, heads=heads, dropout_rate=attn_dropout_rate),\n                        )\n                    ),\n                    Residual(\n                        PreNorm(dim, FeedForward(dim, mlp_dim, dropout_rate))\n                    ),\n                ]\n            )\n        self.net = IntermediateSequential(*layers)\n\n\n    def forward(self, x):\n        return self.net(x)\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:18.975746Z","iopub.execute_input":"2022-05-11T00:40:18.976237Z","iopub.status.idle":"2022-05-11T00:40:18.984708Z","shell.execute_reply.started":"2022-05-11T00:40:18.976201Z","shell.execute_reply":"2022-05-11T00:40:18.983998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import stuff\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset\nfrom glob import glob\nimport nibabel as nib\nfrom torchvision import transforms\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport random\n\nclass MRIDataset():\n\n    def __init__(self, mri_dir, masks_dir, protocol, mode, val_fold, test_fold):\n        self.mode = mode\n        self.mri_dir = mri_dir\n        self.masks_dir = masks_dir\n        self.protocol = protocol\n        self.val_fold = val_fold\n        self.test_fold = test_fold\n        self.train_folds = [1,2,3,4,5]\n\n        self.maskFiles = []\n\n        # This dataloader is only for cross validation, but can be adapted for test as well\n        assert self.mode in ['train','val', 'test', 'gen_outputs']\n\n        if self.mode == 'train':\n            # For this dataset, I did cross validation. If you would like to also have a test fold, uncomment line 35\n\n            self.train_folds.remove(self.test_fold)\n            self.train_folds.remove(self.val_fold)\n\n            # Get lists of the image file paths and each code\n            self.imageFiles, self.imageCodes = self.getpaths(self.train_folds,self.mri_dir, self.protocol, self.mode)\n\n            # Create dictionary of mask file paths corresponding to each image code\n            self.masks_dict = self.getmasks(self.masks_dir, self.protocol, self.imageCodes)\n\n        elif mode == 'val' or mode == 'test' or mode == 'gen_outputs':\n            if mode == 'val':\n                self.imageFiles, self.imageCodes = self.getpaths([self.val_fold], self.mri_dir, self.protocol, self.mode)\n\n            else:\n                self.imageFiles, self.imageCodes = self.getpaths([self.test_fold],self.mri_dir,self.protocol, self.mode)\n\n            self.masks_dict = self.getmasks(self.masks_dir, self.protocol, self.imageCodes)\n\n    def __getitem__(self,idx):\n        image_filepath = self.imageFiles[idx]\n        code = self.imageCodes[idx]\n\n        #now we can load the nifti file and process it\n        image, stacked_masks = self.loadimage(image_filepath, code, self.protocol)\n\n        sample = {'T1': image,\n              'label': stacked_masks,\n              'code': code\n              }\n\n        return sample\n\n    def __len__(self):\n        return len(self.imageFiles)\n\n    def getpaths(self, folds, mri_dir, protocol, mode):\n        imageFiles = []\n        imageCodes = []\n        for fold in folds:\n            # Add image file paths for every image in a given fold\n            imageFiles.extend(sorted(glob(mri_dir + '/Fold_' + str(fold) + '/*.nii')))\n        if protocol == 'dlbs':\n            for path in imageFiles:\n                # The different domains have the codes in slightly different portions of the path\n                imageCodes.append(path[-72:-65])\n        elif protocol == 'SALD':\n            for path in imageFiles:\n                imageCodes.append(path[-51:-45])\n        elif protocol == 'IXI':\n            for path in imageFiles:\n                imageCodes.append(path[-59:-56])\n        elif protocol == 'total':\n            for path in imageFiles:\n                if 'dlbs' in path:\n                    imageCodes.append(path[-72:-65])\n                elif 'SALD' in path:\n                    imageCodes.append(path[-51:-45])\n                elif 'IXI' in path:\n                    imageCodes.append(path[-59:-56])\n        # for testing purposes with dlbs\n        else:\n            for path in imageFiles:\n                imageCodes.append(path[-72:-65])\n        print(\"The {} dataset for domain {} now has {} images\".format(mode, protocol, len(imageFiles)))\n        return imageFiles, imageCodes\n\n    def getmasks(self, masks_dir, protocol, imageCodes):\n        intermediate = []\n        maskFiles = []\n        # Start by making an intermediate list of all the masks paths.\n        intermediate.extend(sorted(glob(masks_dir + '/*.nii')))\n        \n\n        for file in intermediate:\n            if protocol == 'dlbs':\n                cur_code = file[-20:-13]\n            if protocol == 'SALD':\n                cur_code = file[-16:-10]\n            if protocol == 'IXI':\n                cur_code = file[-16:-13]\n            if protocol == 'total':\n                if 'dlbs' in file:\n                    cur_code = file[-20:-13]\n                if 'SALD' in file:\n                    cur_code = file[-16:-10]\n                if 'IXI' in file:\n                    cur_code = file[-16:-13]\n            if cur_code in imageCodes:\n                # Only add the mask files that correspond to images we care about\n                maskFiles.append(file)\n\n        # Create masks dict\n        masks_dict = { i : [] for i in imageCodes }\n        for mask in maskFiles:\n            for code in imageCodes:\n                if code in mask:\n                    masks_dict[code].append(mask)\n        return masks_dict\n\n    def loadimage(self, image_filepath, code, protocol):\n        # Load, splice, and pad image\n        image = nib.load(image_filepath)\n        image = image.slicer[:,15:207,:]\n        image = np.array(image.dataobj)\n        image = np.pad(image, [(5, 5), (0, 0), (5,5)], mode='constant')\n        image = torch.from_numpy(image)\n        image = image.unsqueeze(0)\n        image = image.float()\n        max = torch.max(image)\n        if protocol == 'dlbs':\n            image = 2*(image / max) - 1\n        if protocol == 'IXI':\n            image = 2*(image / max) - 1\n        if protocol == 'SALD' or protocol == 'total':\n            image = 2*(image / max) - 1\n            \n        # -----------------------  Data Augmentation -----------------------\n        transform = tio.RandomAffine(\n            scales=(0.9, 1.2),\n            degrees=15,\n        )\n        image = transform(image)\n        \n        transform = tio.RandomGamma(log_gamma=(-0.3, 0.3))\n        image = transform(image)\n        # ------------------------------------------------------------------\n\n        masks = ['mask_1', 'mask_2', 'mask_3']\n        mask_tensors = { i : {} for i in masks }\n        # Load, slice, and pad each mask. Add them to masks dictionary\n        for num in range(1,4):\n            mask_tensors['mask_'+ str(num)] = nib.load(self.masks_dict[code][num-1])\n            mask_tensors['mask_'+ str(num)] = mask_tensors['mask_'+ str(num)].slicer[:,15:207,:]\n            mask_tensors['mask_'+ str(num)] = np.array(mask_tensors['mask_'+ str(num)].dataobj)\n            mask_tensors['mask_'+ str(num)] = np.pad(mask_tensors['mask_'+ str(num)], [(5, 5), (0, 0), (5, 5)], mode='constant')\n            mask_tensors['mask_'+ str(num)] = torch.from_numpy(mask_tensors['mask_'+ str(num)])\n\n        # Stack all 3 individual brain masks to a single 3 channel GT\n        stacked_masks = torch.stack([mask_tensors['mask_1'], mask_tensors['mask_2'], mask_tensors['mask_3']], dim=0)\n        stacked_masks = stacked_masks.float()\n\n        return image, stacked_masks\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:18.997513Z","iopub.execute_input":"2022-05-11T00:40:18.997701Z","iopub.status.idle":"2022-05-11T00:40:21.148177Z","shell.execute_reply.started":"2022-05-11T00:40:18.997678Z","shell.execute_reply":"2022-05-11T00:40:21.147448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Arg:\n    MRI_dir_SALD = '../input/sald-data/'\n    GT_dir_SALD = '../input/sald-mask/Masks_SALD/'\n    def __init__(self):\n        self.protocol = 'SALD'\n        self.possible_protocols = ['SALD']\n        self.end_epoch = 120\n        self.seed = 1000\n        self.lr = 0.00001\n        self.weight_decay = 1e-5\n        self.amsgrad = True\n        self.val_fold = 5\n        self.test_fold = 1\n        self.batch_size = 1\n        self.num_workers = 2\n        self.start_epoch = 0\n        self.save_root = ''","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:21.149712Z","iopub.execute_input":"2022-05-11T00:40:21.14995Z","iopub.status.idle":"2022-05-11T00:40:21.158205Z","shell.execute_reply.started":"2022-05-11T00:40:21.149917Z","shell.execute_reply":"2022-05-11T00:40:21.157373Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchio","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:21.159542Z","iopub.execute_input":"2022-05-11T00:40:21.15992Z","iopub.status.idle":"2022-05-11T00:40:30.766256Z","shell.execute_reply.started":"2022-05-11T00:40:21.159882Z","shell.execute_reply":"2022-05-11T00:40:30.76544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torchio as tio","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:30.768005Z","iopub.execute_input":"2022-05-11T00:40:30.768262Z","iopub.status.idle":"2022-05-11T00:40:31.565628Z","shell.execute_reply.started":"2022-05-11T00:40:30.768229Z","shell.execute_reply":"2022-05-11T00:40:31.564887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TRAIN CODE\n\n# Import stuff\nimport os\nimport random\nimport logging\nimport numpy as np\nimport time\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.optim\n\nimport torch.distributed as dist\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom skimage.metrics import peak_signal_noise_ratio\nimport nibabel as nib\n\nfrom TABS_Model import TABS\n\n\nargs = Arg()\n\nmodel = TABS()\n# Declare lists to keep track of training and val losses over the epochs\ntrain_global_losses = []\nval_global_losses = []\nbest_epoch = 0\n\ndef main_worker():\n\n    assert args.protocol in args.possible_protocols, 'Protocol must be one of 4 possible protocols: dlbs, SALD, IXI, total'\n\n    if args.protocol == 'total':\n        args.end_epoch = 200\n\n    dirs = {i : [] for i in args.possible_protocols}\n    for protocol in args.possible_protocols:\n        mri_dir = getattr(args, 'MRI_dir_' + protocol)\n        gt_dir = getattr(args, 'GT_dir_' + protocol)\n        dirs[protocol].append(mri_dir)\n        dirs[protocol].append(gt_dir)\n\n    MRI_dir = dirs[args.protocol][0]\n    GT_dir = dirs[args.protocol][1]\n\n    # Set seed\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    print(count_parameters(model))\n    \n    model.cuda()\n\n    print('Model Built!')\n\n    # Using adam optimizer (amsgrad variant) with weight decay\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay, amsgrad=args.amsgrad)\n\n    # MSE loss for this task (regression). Using reduction value of sum because we want to specify the number of voxels to divide by (only in the brain map)\n    criterion = nn.MSELoss(reduction='sum')\n#     criterion = criterion.cuda(args.local_rank)  # not necessary\n    criterion = criterion.cuda()\n\n    # Obtain train dataset\n    Train_MRIDataset = MRIDataset(MRI_dir, GT_dir, protocol=args.protocol, mode='train', val_fold=args.val_fold, test_fold=args.test_fold)\n    \n    # Obtain train dataloader\n    Train_dataloader = DataLoader(Train_MRIDataset, shuffle=True, num_workers=args.num_workers, batch_size=args.batch_size, pin_memory=True)\n\n    # Obtain val dataset\n    Val_MRIDataset = MRIDataset(MRI_dir, GT_dir, protocol=args.protocol, mode='val', val_fold=args.val_fold, test_fold=args.test_fold)\n\n    # Obtain val_dataloader\n    Val_dataloader = DataLoader(Val_MRIDataset, shuffle=False, num_workers=args.num_workers, batch_size=args.batch_size, pin_memory=True)\n\n    start_time = time.time()\n\n    # Enable gradient calculation for training\n    torch.set_grad_enabled(True)\n\n    print('Start to train!')\n\n    # Main training/validation loop\n    for epoch in range(args.start_epoch, args.end_epoch):\n\n        # Declare lists to keep track of losses and metrics within the epoch\n        train_epoch_losses = []\n        val_epoch_losses = []\n        val_epoch_pcorr = []\n        val_epoch_psnr = []\n        start_epoch = time.time()\n\n        model.train()\n\n        count = 0\n        for i, data in enumerate(Train_dataloader):\n            # break\n\n            adjust_learning_rate(optimizer, epoch, args.end_epoch, args.lr)\n\n            mri_images = data['T1']\n            targets = data['label']\n\n            mri_images = mri_images.cuda(non_blocking=True)\n            targets = targets.cuda(non_blocking=True)\n\n            loss, isolated_images, stacked_brain_map  = get_loss(model, criterion, mri_images, targets, 'train')\n\n            train_epoch_losses.append(loss.item())\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        # Transition to val mode\n        model.eval()\n\n        # Avoid computing gradients during validation to save memory\n        with torch.no_grad():\n\n            for i, data in enumerate(Val_dataloader):\n\n                mri_images = data['T1']\n                targets = data['label']\n\n                mri_images = mri_images.cuda(non_blocking=True)\n                targets = targets.cuda(non_blocking=True)\n\n                loss, isolated_images, stacked_brain_map  = get_loss(model, criterion, mri_images, targets, 'val')\n\n                val_epoch_losses.append(loss.item())\n\n                # Calculate the pearson correlation between the output and ground truth only for the voxels of the brain map.\n\n                for g in range(0,len(isolated_images)):\n                    cur_pcorr, cur_psnr = overall_metrics(isolated_images[g], targets[g], stacked_brain_map[g])\n                    val_epoch_pcorr.append(cur_pcorr)\n                    val_epoch_psnr.append(cur_psnr)\n\n        end_epoch = time.time()\n\n        # Average train and val loss over every MRI scan in the epoch. Save to global losses which tracks across epochs\n        train_net_loss = sum(train_epoch_losses) / len(train_epoch_losses)\n        val_net_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n        train_global_losses.append(train_net_loss)\n        val_global_losses.append(val_net_loss)\n        # Average pearson correlation and psnr over the epochs\n        psnr = sum(val_epoch_psnr) / len(val_epoch_psnr)\n        pcorr = sum(val_epoch_pcorr) / len(val_epoch_pcorr)\n\n        print('Epoch: {} | Train Loss: {} | Val Loss: {} | PSNR: {} | Pearson: {}'.format(epoch, train_net_loss, val_net_loss, psnr, pcorr))\n\n        checkpoint_dir = args.save_root\n        # Save the model if it reaches a new min validation loss\n        if val_global_losses[-1] == min(val_global_losses):\n            print('saving model at the end of epoch ' + str(epoch))\n            best_epoch = epoch\n            file_name = os.path.join(checkpoint_dir, 'ResTransUNet3D_model_epoch_{}.pth'.format(epoch, val_global_losses[-1]))\n            if epoch > 150:\n                torch.save({\n                    'epoch': epoch,\n                    'state_dict': model.state_dict(),\n                    'optim_dict': optimizer.state_dict(),\n                    },\n                    file_name)\n\n    end_time = time.time()\n    total_time = (end_time - start_time) / 3600\n    print('The total training time is {:.2f} hours'.format(total_time))\n\n    print('----------------------------------The training process finished!-----------------------------------')\n\n    log_name = os.path.join(args.save_root, 'loss_log_restransunet.txt')\n\n    with open(log_name, \"a\") as log_file:\n        now = time.strftime(\"%c\")\n        log_file.write('================ Loss (%s) ================\\n' % now)\n        log_file.write('best_epoch: ' + str(best_epoch) + '\\n')\n        log_file.write('train_losses: ')\n        log_file.write('%s\\n' % train_global_losses)\n        log_file.write('val_losses: ')\n        log_file.write('%s\\n' % val_global_losses)\n        log_file.write('train_time: ' + str(total_time))\n\n    learning_curve(best_epoch, train_global_losses, val_global_losses)\n\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n# Input the best epoch, lists of global (across epochs) train and val losses. Plot learning curve\ndef learning_curve(best_epoch, train_global_losses, val_global_losses):\n    fig, ax1 = plt.subplots(figsize=(12, 8))\n\n    ax1.set_xlabel('Epochs')\n    ax1.set_xticks(np.arange(0, int(len(train_global_losses) + 1), 10))\n\n    ax1.set_ylabel('Loss')\n    ax1.plot(train_global_losses, '-r', label='Training loss', markersize=3)\n    ax1.plot(val_global_losses, '-b', label='Validation loss', markersize=3)\n    ax1.axvline(best_epoch, color='m', lw=4, alpha=0.5, label='Best epoch')\n    ax1.legend(loc='upper left')\n    save_name = 'Learning_Curve_restransunet3d_' + args.protocol + '_' + str(args.val_fold) + '.png'\n    plt.savefig(os.path.join(args.save_root, save_name))\n\ndef adjust_learning_rate(optimizer, epoch, max_epoch, init_lr, power=0.9):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = round(init_lr * np.power(1 - (epoch / max_epoch), power), 8)\n\n# Calculate pearson correlation and psnr only between the voxels of the brain map (do by total brain not tissue type during training)\ndef overall_metrics(isolated_image, target, stacked_brain_map):\n    # Flatten the GT, isolated output, and brain mask\n    GT_flattened = torch.flatten(target)\n    iso_flattened = torch.flatten(isolated_image)\n    mask_flattened = torch.flatten(stacked_brain_map)\n\n    # Only save the part of the flattened GT/output that corresponds to nonzero values of the brain mask\n    GT_flattened = GT_flattened[mask_flattened.nonzero(as_tuple=True)]\n    iso_flattened = iso_flattened[mask_flattened.nonzero(as_tuple=True)]\n\n    iso_flattened = iso_flattened.cpu().detach().numpy()\n    GT_flattened = GT_flattened.cpu().detach().numpy()\n\n    pearson = np.corrcoef(iso_flattened, GT_flattened)[0][1]\n    psnr = peak_signal_noise_ratio(iso_flattened, GT_flattened)\n\n    return pearson, psnr\n\n# Given the model, criterion, input, and GT, this function calculates the loss and returns the isolated output (stripped of background) and brain map\ndef get_loss(model, criterion, mri_images, targets, mode):\n\n    if mode == 'val':\n        torch.manual_seed(args.seed)\n        torch.cuda.manual_seed(args.seed)\n        random.seed(args.seed)\n        np.random.seed(args.seed)\n\n    # Get brain map from FSL GT by taking all values >0 --> 1\n    output = model(mri_images)\n\n    test = torch.squeeze(mri_images,dim=1)\n    brain_map = (test > -1).float()\n\n    stacked_brain_map = torch.stack([brain_map, brain_map, brain_map], dim=1)\n\n    isolated_images = torch.mul(stacked_brain_map, output)\n\n    loss = criterion(isolated_images, targets)\n    num_brain_voxels = stacked_brain_map.sum()\n    loss = loss / num_brain_voxels\n\n    return loss, isolated_images, stacked_brain_map\n\nif __name__ == '__main__':\n    assert torch.cuda.is_available(), \"Currently, we only support CUDA version\"\n    torch.backends.cudnn.enabled = True\n    torch.backends.cudnn.benchmark = True\n    main_worker()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-11T00:40:31.567833Z","iopub.execute_input":"2022-05-11T00:40:31.568043Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install MedPy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## TEST CODE\n\nimport numpy as np\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport random\nfrom medpy.metric.binary import hd\nimport nibabel as nib\nimport os\n\nfrom torch.utils.data import DataLoader\nfrom skimage.metrics import peak_signal_noise_ratio\nfrom scipy.stats import spearmanr\nfrom sklearn.metrics import jaccard_score\nfrom TABS_Model import TABS\n\ndef get_loss(model, criterion, mri_images, targets, mode):\n\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    # Get brain map from FSL GT by taking all values >0 --> 1\n    output = model(mri_images)\n\n    test = torch.squeeze(mri_images,dim=1)\n    brain_map = (test > -1).float()\n\n    stacked_brain_map = torch.stack([brain_map, brain_map, brain_map], dim=1)\n\n    isolated_images = torch.mul(stacked_brain_map, output)\n\n    loss = criterion(isolated_images, targets)\n    num_brain_voxels = stacked_brain_map.sum()\n    loss = loss / num_brain_voxels\n\n    return loss, isolated_images, stacked_brain_map\n\ndef tissue_wise_probability_metrics(isolated_image, target, stacked_brain_map):\n    criterion = nn.MSELoss()\n    criterion = criterion.cuda()\n\n    # metrics dict to store metric for each tissue type\n    metrics_list = ['pearson_corr', 'spearman_corr', 'psnr', 'mse']\n    metrics = { i : [] for i in metrics_list }\n\n    # list of flattened tensors I'm gonna collect and their corresponding dict\n    necessary_flattened_tensors = ['GT_flattened_0', 'GT_flattened_1', 'GT_flattened_2', 'iso_flattened_0', 'iso_flattened_1', 'iso_flattened_2']\n    flattened_tensors = { i : {} for i in necessary_flattened_tensors }\n\n    # flattened single channel brain mask (192x192x192 --> flat)\n    mask_flattened = torch.flatten(stacked_brain_map[0])\n\n    # Only save the part of the flattened GT/output that correspond to nonzero values of the brain mask\n    for i in range(0,3):\n        # flatten gt of channel i (each channel corresponds to a tissue type)\n        flattened_tensors['GT_flattened_' + str(i)] = torch.flatten(target[i])\n        # choose only the portion of the flattened gt that correspons to the brain\n        flattened_tensors['GT_flattened_' + str(i)] = flattened_tensors['GT_flattened_' + str(i)][mask_flattened.nonzero(as_tuple=True)]\n        # make this now a numpy array\n        flattened_tensors['GT_flattened_' + str(i)] = flattened_tensors['GT_flattened_' + str(i)].cpu().detach().numpy()\n\n        # repeat for the model output image\n        flattened_tensors['iso_flattened_' + str(i)] = torch.flatten(isolated_image[i])\n        flattened_tensors['iso_flattened_' + str(i)] = flattened_tensors['iso_flattened_' + str(i)][mask_flattened.nonzero(as_tuple=True)]\n        flattened_tensors['iso_flattened_' + str(i)] = flattened_tensors['iso_flattened_' + str(i)].cpu().detach().numpy()\n\n    for i in range(0,3):\n        # get output and gt from dict i just constructed\n        model_output = flattened_tensors['iso_flattened_' + str(i)]\n        GT = flattened_tensors['GT_flattened_' + str(i)]\n\n        # get metrics using the numpy arrays of both (cropped to brain)\n        cur_pcorr = np.corrcoef(model_output, GT)[0][1]\n        cur_scorr = spearmanr(model_output, GT)[0]\n        cur_psnr = peak_signal_noise_ratio(model_output, GT)\n\n        cur_mse = criterion(torch.tensor(model_output).cuda(), torch.tensor(GT).cuda())\n\n        metrics['pearson_corr'].append(cur_pcorr)\n        metrics['spearman_corr'].append(cur_scorr)\n        metrics['psnr'].append(cur_psnr)\n        metrics['mse'].append(cur_mse.item())\n\n    return metrics\n\ndef tissue_wise_map_metrics(isolated_image, target, stacked_brain_map):\n    # metrics dict to store metric for each tissue type\n    metrics_list = ['DICE', 'HD', 'Jaccard']\n    metrics = { i : [] for i in metrics_list }\n\n    # list of flattened tensors (segmentation masks) I'm gonna collect and their corresponding dict\n    necessary_masks_list = ['GT_0', 'GT_1', 'GT_2', 'iso_0', 'iso_1', 'iso_2']\n    necessary_tensors = { i : {} for i in necessary_masks_list }\n\n    # current output and gt is 3x192x192x192. Basically, each voxel of the brain has 3 probabilities assigned to it for each tissue type. Taking the argmax gives us the most likely tissue type of each voxel (now 1x192x192x192)\n    full_map_model = torch.argmax(isolated_image,0)\n    full_map_GT = torch.argmax(target,0)\n    mask = stacked_brain_map[0]\n    mask_flattened = torch.flatten(stacked_brain_map[0])\n\n    for i in range(0,3):\n        # now that we have the argmax, we can imagine the brain with each voxel having a value of 0,1,2. To get the masks for each tissue type, we save a new tensor corresponding to 1 where the argmax tensor has a value of the given tissue type and 0 otherwise.\n        necessary_tensors['GT_' + str(i)] = (full_map_GT==i).float()\n        necessary_tensors['iso_' + str(i)] = (full_map_model==i).float()\n        if i == 0:\n            # make sure background is 0\n            necessary_tensors['GT_' + str(i)] = torch.mul(necessary_tensors['GT_' + str(i)], mask)\n            necessary_tensors['iso_' + str(i)] = torch.mul(necessary_tensors['iso_' + str(i)], mask)\n\n        # calc HD with the segmentation masks\n        h_dist = hd(necessary_tensors['iso_' + str(i)].cpu().detach().numpy(), necessary_tensors['GT_' + str(i)].cpu().detach().numpy())\n        metrics['HD'].append(h_dist)\n\n        # now make cropped 1d numpy arrays only containing mask values for within the brain for dice calculation\n        necessary_tensors['GT_' + str(i)] = torch.flatten(necessary_tensors['GT_' + str(i)])\n        necessary_tensors['GT_' + str(i)] = necessary_tensors['GT_' + str(i)][mask_flattened.nonzero(as_tuple=True)]\n        necessary_tensors['GT_' + str(i)] = necessary_tensors['GT_' + str(i)].cpu().detach().numpy()\n        necessary_tensors['iso_' + str(i)] = torch.flatten(necessary_tensors['iso_' + str(i)])\n        necessary_tensors['iso_' + str(i)] = necessary_tensors['iso_' + str(i)][mask_flattened.nonzero(as_tuple=True)]\n        necessary_tensors['iso_' + str(i)] = necessary_tensors['iso_' + str(i)].cpu().detach().numpy()\n\n    for i in range(0,3):\n        model_output = necessary_tensors['iso_' + str(i)]\n        GT = necessary_tensors['GT_' + str(i)]\n        # dice formula\n        dice = np.sum(model_output[GT==1])*2.0 / (np.sum(model_output) + np.sum(GT))\n        jaccard = jaccard_score(GT, model_output)\n\n        metrics['DICE'].append(dice)\n        metrics['Jaccard'].append(jaccard)\n\n    return metrics\n\nif __name__ == '__main__':\n\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed(args.seed)\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n\n    dirs = { i : [] for i in args.possible_protocols}\n    for protocol in args.possible_protocols:\n        mri_dir = getattr(args, 'MRI_dir_' + protocol)\n        gt_dir = getattr(args, 'GT_dir_' + protocol)\n        dirs[protocol].append(mri_dir)\n        dirs[protocol].append(gt_dir)\n\n    MRI_dir = dirs[args.protocol][0]\n    GT_dir = dirs[args.protocol][1]\n\n    Test_MRIDataset = MRIDataset(MRI_dir, GT_dir, protocol=args.protocol, mode='test', val_fold = 5, test_fold = args.test_fold)\n\n    Test_dataloader = DataLoader(Test_MRIDataset, shuffle=False, num_workers=args.num_workers, batch_size=args.batch_size, pin_memory=False)\n\n    criterion = nn.MSELoss(reduction='sum')\n    criterion = criterion.cuda()\n\n    probability_metrics_list = ['pearson_corr', 'spearman_corr', 'psnr', 'mse']\n    probability_metrics = { i : [] for i in probability_metrics_list }\n    map_metrics_list = ['DICE', 'HD', 'Jaccard']\n    map_metrics = { i : [] for i in map_metrics_list }\n\n    model.eval()\n\n    with torch.no_grad():\n        val_losses = []\n        test = []\n        val_psnr = []\n        val_corr = []\n\n        count = 0\n        for i, data in enumerate(Test_dataloader):\n\n            mri_images = data['T1']\n            targets = data['label']\n            samples = data['code']\n\n            mri_images = mri_images.cuda(non_blocking=True)\n            targets = targets.cuda(non_blocking=True)\n\n            loss, isolated_images, stacked_brain_maps  = get_loss(model, criterion, mri_images, targets, 'val')\n\n            val_losses.append(loss)\n            #\n            for g in range(0,len(isolated_images)):\n                isolated_image = isolated_images[g]\n                target = targets[g]\n                stacked_brain_map = stacked_brain_maps[g]\n                metrics_maps = tissue_wise_map_metrics(isolated_image, target, stacked_brain_map)\n                metrics =  tissue_wise_probability_metrics(isolated_image, target, stacked_brain_map)\n\n                for metric in probability_metrics_list:\n                    probability_metrics[metric].append(metrics[metric])\n                for metric in map_metrics_list:\n                    map_metrics[metric].append(metrics_maps[metric])\n\n    val_net_loss = sum(val_losses)/len(val_losses)\n\n    overall_pcorr = probability_metrics['pearson_corr']\n    overall_pcorr = np.array(overall_pcorr)\n    avg_pcorr = sum(overall_pcorr)/len(overall_pcorr)\n    sd_pcorr = np.std(overall_pcorr, axis=0, ddof=1)\n\n    overall_scorr = probability_metrics['spearman_corr']\n    overall_scorr = np.array(overall_scorr)\n    avg_scorr = sum(overall_scorr)/len(overall_scorr)\n    sd_scorr = np.std(overall_scorr, axis=0, ddof=1)\n\n    overall_psnr = probability_metrics['psnr']\n    overall_psnr = np.array(overall_psnr)\n    avg_psnr = sum(overall_psnr)/len(overall_psnr)\n    sd_psnr = np.std(overall_psnr, axis=0, ddof=1)\n\n    overall_mse = probability_metrics['mse']\n    overall_mse = np.array(overall_mse)\n    avg_mse = sum(overall_mse)/len(overall_mse)\n    sd_mse = np.std(overall_mse, axis=0, ddof=1)\n\n    overall_DICE = map_metrics['DICE']\n    overall_DICE = np.array(overall_DICE)\n    avg_DICE = sum(overall_DICE)/len(overall_DICE)\n    sd_DICE = np.std(overall_DICE, axis=0, ddof=1)\n\n    overall_HD = map_metrics['HD']\n    overall_HD = np.array(overall_HD)\n    avg_HD = sum(overall_HD)/len(overall_HD)\n    sd_HD = np.std(overall_HD, axis=0, ddof=1)\n\n    overall_jaccard = map_metrics['Jaccard']\n    overall_jaccard = np.array(overall_jaccard)\n    avg_jaccard = sum(overall_jaccard)/len(overall_jaccard)\n    sd_jaccard = np.std(overall_jaccard, axis=0, ddof=1)\n\n    print('Probability-Based Metrics:')\n    print('Val Loss: {} | Pearson: {} SD: {} | Spearman: {} SD: {} | psnr: {} SD: {} | MSE: {} SD: {} |'.format(val_net_loss, avg_pcorr, sd_pcorr, avg_scorr, sd_scorr, avg_psnr, sd_psnr, avg_mse, sd_mse))\n\n    print('Map-Based Metrics:')\n    print('DICE: {} SD: {} | HD: {} SD: {} | Jaccard: {} SD: {}'.format(avg_DICE, sd_DICE, avg_HD, sd_HD, avg_jaccard, sd_jaccard))\n\n    log_name = os.path.join(args.save_root, 'test_restransunet_ixi.txt')\n    with open(log_name, \"a\") as log_file:\n        log_file.write('Pearson: {} SD: {} | Spearman: {} SD: {} | psnr: {} SD: {} | MSE: {} SD: {} |'.format(avg_pcorr, sd_pcorr, avg_scorr, sd_scorr, avg_psnr, sd_psnr, avg_mse, sd_mse))\n        log_file.write('\\n')\n        log_file.write('DICE: {} SD: {} | HD: {} SD: {} | Jaccard: {} SD: {}'.format(avg_DICE, sd_DICE, avg_HD, sd_HD, avg_jaccard, sd_jaccard))\n        log_file.write('\\n')\n        log_file.write('pcorr')\n        log_file.write('%s\\n' % overall_pcorr)\n        log_file.write('scorr')\n        log_file.write('%s\\n' % overall_scorr)\n        log_file.write('MSE')\n        log_file.write('%s\\n' % overall_mse)\n        log_file.write('dice')\n        log_file.write('%s\\n' % overall_DICE)\n        log_file.write('jaccard')\n        log_file.write('%s\\n' % overall_jaccard)\n        log_file.write('hd')\n        log_file.write('%s\\n' % overall_HD)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}